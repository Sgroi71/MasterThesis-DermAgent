{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1feece6",
   "metadata": {},
   "source": [
    "Attention-based Explainable AI (XAI)\n",
    "========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a685f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "ROOT = ...\n",
    "sys.path.append(ROOT)\n",
    "\n",
    "\n",
    "from medDerm.agent import *\n",
    "from medDerm.tools import *\n",
    "from medDerm.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b5a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda\"\n",
    "config_path=f\"{ROOT}/checkpoints/exp-HAM+Derm7pt-all+BCN+HAM-bin+DermNet+Fitzpatrick.yaml\"\n",
    "\n",
    "\n",
    "torch.mps.empty_cache()\n",
    "model = load_checkpoint(config_path).to(device)\n",
    "model.eval()\n",
    "head=\"HAM10k\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fa5ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions = []\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    # output is (batch_size, num_heads, tokens, tokens)\n",
    "    attentions.append(output)\n",
    "\n",
    "layer = 2\n",
    "\n",
    "# Register the hook for each transformer block\n",
    "model_info=model.model\n",
    "block = model_info.layers[layer-1]\n",
    "for transformer_block in block.blocks:\n",
    "    transformer_block.attn.attn_drop.register_forward_hook(hook_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5376dc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess image\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "])\n",
    "image_path = f\"{ROOT}/datasets/ISIC2018_Task3_Test_input/ISIC_0035859.jpg\"  # Replace with your image path\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = model.forward_explaination_tasks(input_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cc243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, att in enumerate(attentions):\n",
    "    print(f\"Layer {i}: attention shape = {att.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed051e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def compute_rollout_attention(attentions, start_layer=4, end_layer=22, skip_layers=False):# fix the layer in order to use only layers with the same batch size\n",
    "    # Use the average across all heads\n",
    "    attentions = attentions[start_layer:end_layer]\n",
    "    batch_size = attentions[0].size(0)\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    for i,att in enumerate(attentions):\n",
    "        if att.size(0) != batch_size:\n",
    "            if not skip_layers:\n",
    "                print(f\"Layer {i} has different batch size: {att.size(0)} select the right interval of layers or call the function with the parameter skip_layers=True\")\n",
    "                return None\n",
    "            else: \n",
    "                print(f\"Layer {i} has different batch size: {att.size(0)}, erased from attentions\")\n",
    "                attentions.pop(i)\n",
    "\n",
    "    result = torch.eye(attentions[0].size(-1)).to(device) # Identity matrix N*N (N number of tokens)\n",
    "    for attention in attentions:\n",
    "        #each attention is (batch_size, num_heads, tokens, tokens)\n",
    "        attention_heads_fused = attention.mean(dim=1) # mean over heads-> (batch_size, tokens, tokens)\n",
    "        attention_heads_fused += torch.eye(attention_heads_fused.size(-1)).to(device) # Add identity matrix in order to avoid zero attention\n",
    "        attention_heads_fused /= attention_heads_fused.sum(dim=-1, keepdim=True)#Normalizes the attention across each row (so each token's attentions sum to 1).\n",
    "        result = torch.matmul(attention_heads_fused, result)\n",
    "    return result[0]\n",
    "\n",
    "rollout = compute_rollout_attention(attentions, start_layer=4, end_layer=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12a5f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Assuming the attention map is for 7x7 patches\n",
    "heatmap = rollout[1:].mean(dim=0).reshape(7, 7).detach().cpu().numpy()\n",
    "heatmap = cv2.resize(heatmap, (224, 224))\n",
    "heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
    "\n",
    "# Overlay on image\n",
    "img_np = np.array(image.resize((224, 224)))\n",
    "heatmap_color = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
    "overlay = heatmap_color * 0.3 + img_np * 0.7\n",
    "\n",
    "plt.imshow(overlay.astype(np.uint8))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
