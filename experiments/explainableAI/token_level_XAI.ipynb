{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1feece6",
   "metadata": {},
   "source": [
    "Attention-based Explainable AI (XAI)\n",
    "========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a685f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "ROOT = ...\n",
    "sys.path.append(ROOT)\n",
    "\n",
    "\n",
    "from medDerm.agent import *\n",
    "from medDerm.tools import *\n",
    "from medDerm.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b5a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda\"\n",
    "config_path=f\"{ROOT}/checkpoints/exp-HAM+Derm7pt-all+BCN+HAM-bin+DermNet+Fitzpatrick.yaml\"\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model = load_checkpoint(config_path).to(device)\n",
    "model.eval()\n",
    "head=\"HAM10k\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fa5ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {}\n",
    "gradients = {}\n",
    "\n",
    "level=0\n",
    "\n",
    "model_info= model.model\n",
    "target_layer = model_info.layers[level].blocks[-1].norm2  # or attn.proj if you prefer\n",
    "\n",
    "def forward_hook(module, input, output):\n",
    "    activations[\"value\"] = output.detach()\n",
    "\n",
    "def backward_hook(module, grad_input, grad_output):\n",
    "    gradients[\"value\"] = grad_output[0].detach()\n",
    "\n",
    "handle_f = target_layer.register_forward_hook(forward_hook)\n",
    "handle_b = target_layer.register_backward_hook(backward_hook)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5376dc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "])\n",
    "image_path = f\"{ROOT}/datasets/ISIC2018_Task3_Test_input/ISIC_0035859.jpg\"  # Replace with your image path\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "output = model.forward_attention_tasks(input_tensor)\n",
    "class_idx = output.argmax(dim=1).item()\n",
    "score = output[0, class_idx]\n",
    "model.zero_grad()\n",
    "score.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed051e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grad-CAM style weights\n",
    "weights = gradients[\"value\"].mean(dim=1, keepdim=True)  # average across features\n",
    "\n",
    "# Token-wise importance\n",
    "cam = (weights * activations[\"value\"]).sum(dim=-1).squeeze()  # (num_tokens,)\n",
    "cam = torch.relu(cam)\n",
    "cam = cam / cam.max()  # normalize\n",
    "\n",
    "print(\"cam shape\", cam.shape)\n",
    "\n",
    "if level==3:\n",
    "    cam_image = cam.reshape(7, 7).cpu().numpy()\n",
    "elif level==2:\n",
    "    cam_image = cam.reshape(14, 14).cpu().numpy()\n",
    "else:\n",
    "    cam_image = cam.reshape(int(math.sqrt(cam.shape[0])), int(math.sqrt(cam.shape[0]))).cpu().numpy()\n",
    "\n",
    "\n",
    "handle_f.remove()\n",
    "handle_b.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12a5f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cam_np = cam_image\n",
    "# Normalize again (in case)\n",
    "cam_np = (cam_np - cam_np.min()) / (cam_np.max() - cam_np.min() + 1e-6)\n",
    "# Resize to 224x224\n",
    "cam_resized = cv2.resize(cam_np, (224, 224))\n",
    "\n",
    "heatmap = cv2.applyColorMap(np.uint8(255 * cam_resized), cv2.COLORMAP_JET)\n",
    "heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)  # OpenCV uses BGR\n",
    "\n",
    "img_np = np.array(image.resize((224, 224)))\n",
    "\n",
    "# Blend the image and heatmap (alpha controls transparency)\n",
    "overlayed_img = np.uint8(0.6 * img_np + 0.4 * heatmap)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(overlayed_img)\n",
    "plt.axis('off')\n",
    "plt.title(\"Grad-CAM Heatmap Overlay\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
